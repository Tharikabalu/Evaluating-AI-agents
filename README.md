# Evaluating-AI-agents

Learn how to systematically assess, debug, and improve the performance of your AI agents using observability, structured experiments, and component-wise evaluations.

- **Observability for AI Agents:**  
  Gain insights into your agent’s steps by adding observability. Learn how to collect and visualize traces to debug and understand agent behavior.

- **Systematic Evaluations:**  
  Understand how to prepare testing examples, choose the appropriate evaluator (code-based or LLM-as-a-Judge), and determine the right metrics for assessing each component of your agent.

- **Structured Experiments:**  
  Learn to structure your evaluations into experiments that help you iterate on and improve both the output quality and the logical path of your agent.

- **Component-Wise Evaluation:**  
  Evaluate the performance of individual components like routers, skills, and memory alongside the end-to-end performance of the AI agent.

  
When building an AI agent—be it a shopping assistant, coding agent, or research assistant—a systematic evaluation process is key. This course provides you with the tools and methodologies to:

- Distinguish between evaluating LLM-based systems and traditional software testing.
- Build an AI agent from scratch by understanding its basic structure and components.
- Add observability by tracing your agent’s internal operations.
- Set up evaluations with both code-based methods and LLM-as-a-Judge, supplemented by human annotations.
- Compute convergence scores to determine if your agent responds efficiently within a given query.
- Run structured experiments to test changes in prompts, LLM models, or agent logic.
- Deploy evaluation techniques to monitor your agent's performance in production.
